from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

import logging, os
import pandas as pd
# Lớp này được sử dụng để tạo tạm thời các tệp tin mà tự động được xóa khi chúng không còn được sử dụng.
from tempfile import TemporaryDirectory
from sqlalchemy import create_engine
from docker.types import Mount

default_args = {
    'owner': 'vohoang',
    'retries': 5,
    'retry_delay': timedelta(minutes=2)
}

def extract_data_from_minio(file_name):
    s3_hook = S3Hook(aws_conn_id="minio_conn_id")
    
    # Download file from MinIO to /tmp directory
    s3_hook.download_file(
        key=file_name,
        bucket_name="dbt",
        preserve_file_name=True,
        use_autogenerated_subdir=False
    )
        
    logging.info("Successfully downloaded file from MinIO.")
    
    # Read Excel file into DataFrame
    df = pd.read_excel(f'/tmp/{file_name}')
    
    logging.info("Successfully read data with Pandas.")
    
    # Create connection to PostgreSQL database
    postgres_hook = PostgresHook(postgres_conn_id="postgres_conn_id")
    sqlalchemy_url = postgres_hook.get_uri()
    
    engine = create_engine(sqlalchemy_url)
    
    logging.info("Successfully created SQLAlchemy engine.")
    
    # Insert data into PostgreSQL
    df.to_sql('report', engine, if_exists='replace', index=False)
    
    logging.info("Successfully imported data to PostgreSQL.")
    
    os.remove(f'/tmp/{file_name}')
    
    logging.info("Successfully delete tmp file.")
    

with DAG(
    dag_id="elt_data_from_minio_v31",
    default_args=default_args,
    start_date=datetime(2024, 7, 4),
    schedule_interval=None
) as dag:
    task1 = PythonOperator(
        task_id="extract_load_data_from_minio",
        python_callable=extract_data_from_minio,
        op_kwargs={'file_name': 'SuperStoreUS-2015.xlsx'}
    )
    task2 = DockerOperator(
        task_id="transfrom_data_by_dbt",
        image='ghcr.io/dbt-labs/dbt-postgres:1.7.17',
        command=[
            "run",
            "--profiles-dir",
            "/root",
            "--project-dir",
            "/dbt",
            "--full-refresh"
        ],
        auto_remove=True,
        docker_url="tcp://docker-socket-proxy:2375",
        network_mode="bridge",
        mounts=(
            Mount(source='/home/vohoang/project/inda/dbt/src/transform_data', target='/dbt', type='bind'),
            Mount(source='/home/vohoang/project/inda/dbt/.dbt', target='/root', type='bind')
        ),
        dag=dag
    )
    task1 >> task2
